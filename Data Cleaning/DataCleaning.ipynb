{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataCleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsxDX1qoUatX",
        "outputId": "bf0aa7a3-ba54-49d8-cad5-30e40d509deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnIdU2CDt4Z"
      },
      "source": [
        "#Cleaning English Training Data\n",
        "filename= '/content/IITB.en-hi.en'\n",
        "file=open(filename,'rt')\n",
        "f2=open(\"/content/cleaned_train_en\",\"w+\")\n",
        "\n",
        "while True :\n",
        "    #print(\"a\")\n",
        "    text=file.readline()\n",
        "    if not text:\n",
        "        break\n",
        "    # split into words\n",
        "    \n",
        "    tokens = text.split()\n",
        "    # convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each word\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    \n",
        "    f2.write(\" \".join(words))\n",
        "    f2.write(\"\\n\")\n",
        "#print(text)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7anVa6L9rMy"
      },
      "source": [
        "#Cleaning Hindi Training Data\n",
        "filename= '/content/IITB.en-hi.hi'\n",
        "file=open(filename,'rt')\n",
        "f2=open(\"cleaned_train_hi\",\"w+\")\n",
        "\n",
        "while True :\n",
        "    #print(\"a\")\n",
        "    text=file.readline()\n",
        "    if not text:\n",
        "        break\n",
        "        text=text.replace(\"'\",\"\")\n",
        "        text=text.replace(\"ред\",\"\")\n",
        "        text=text.replace(\",\",\"\")\n",
        "        text=text.replace(\"\\\"\",\"\")\n",
        "        text=text.replace(\".\",\"\")\n",
        "        text=text.replace(\"!\",\"\")\n",
        "        text=text.replace(\"?\",\"\")\n",
        "        text=text.replace(\"(\",\"\")\n",
        "        text=text.replace(\")\",\"\")\n",
        "        text=text.replace(\"[\",\"\")\n",
        "        text=text.replace(\"]\",\"\")\n",
        "        text=text.replace(\"-\",\"\")\n",
        "        text=text.replace(\"_\",\"\")\n",
        "        text=text.replace(\"{\",\"\")\n",
        "        text=text.replace(\"}\",\"\")\n",
        "        text=text.replace(\"%\",\"\")\n",
        "        text=text.replace(\">>\",\"\")\n",
        "        text=text.replace(\"@\",\"\")\n",
        "        text=text.replace(\"/\",\"\")\n",
        "        text=text.replace(\"~\",\"\")\n",
        "        text=text.replace(\"=\",\"\")\n",
        "        text=text.replace(\"&\",\"\")\n",
        "        text=text.replace(\"..\",\"\")\n",
        "        text=text.replace(\"...\",\"\")\n",
        "        text=text.replace(\"┬й\",\"\")\n",
        "        text=text.replace(\";\",\"\")\n",
        "        text=text.replace(\"\\\\\",\"\")\n",
        "        text=text.replace(\":\",\"\")\n",
        "        text=text.replace(\"#\",\"\")\n",
        "        text=text.replace(\"<\",\"\")\n",
        "        text=text.replace(\">\",\"\")\n",
        "        text=re.sub('\\d', '', text)\n",
        "        text=re.sub(\"[a-zA-Z]\",\"\",text)\n",
        "        text=text.replace(\" : \",\" \")\n",
        "        text=text.replace(\"  \",\" \")\n",
        "        text=text.replace(\"   \",\" \")\n",
        "        text=text.strip()\n",
        "        text=text.replace(\"\\n\",\"\")\n",
        "        text=text.replace(\"\\t\",\"\")\n",
        "    f2.write(text)\n",
        "    f2.write(\"\\n\")\n",
        "    \n",
        "    print(text)\n",
        "#print(text)\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaWNgdMcutQW"
      },
      "source": [
        "# Exploratory Data Analysis of the Dataset\n",
        "f1=open(\"drive/My Drive/NLP/CleanedData/cleaned_train_en\",\"rt\",encoding=\"utf8\")\n",
        "f2=open(\"drive/My Drive/NLP/CleanedData/cleaned_test_en\",\"rt\",encoding=\"utf8\")\n",
        "f3=open(\"drive/My Drive/NLP/CleanedData/cleaned_dev_en\",\"rt\",encoding=\"utf8\")\n",
        "unique_tokens=set()\n",
        "row_count=0\n",
        "while True:\n",
        "    text=f1.readline()\n",
        "    if not text:\n",
        "        break\n",
        "    text=text.split()\n",
        "    for tokens in text:\n",
        "        unique_tokens.add(tokens)\n",
        "    row_count=row_count+1\n",
        "\n",
        "print(f\"Vocab len for eng train {len(unique_tokens)}\")\n",
        "\n",
        "not_in_train_is_in_test = set()\n",
        "count_present=0\n",
        "count_not_present=0\n",
        "test_set=set()\n",
        "while True:\n",
        "    text=f2.readline()\n",
        "    if not text:\n",
        "        break\n",
        "    text=text.split()\n",
        "    for token in text:\n",
        "        test_set.add(token)\n",
        "\n",
        "for vocab_token in test_set:\n",
        "    if vocab_token in unique_tokens:\n",
        "        count_present=count_present+1\n",
        "    else:\n",
        "        not_in_train_is_in_test.add(vocab_token)\n",
        "        count_not_present=count_not_present+1\n",
        "    #row_count=row_count+1\n",
        "    \n",
        "\n",
        "\n",
        "print(f\"Total Vocab in Test Eng {len(test_set)}\")\n",
        "print(f\"Test Vocab matching with train Eng {count_present}\")\n",
        "print(f\"Test Vocab not matching with train Eng {len(not_in_train_is_in_test)}\")\n",
        "\n",
        "\n",
        "count_present=0\n",
        "count_not_present=0\n",
        "dev_set=set()\n",
        "while True:\n",
        "    text=f3.readline()\n",
        "    if not text:\n",
        "        break\n",
        "    text=text.split()\n",
        "    for token in text:\n",
        "        dev_set.add(token)\n",
        "    \n",
        "for vocab_token in dev_set:\n",
        "    if vocab_token in unique_tokens:\n",
        "        count_present=count_present+1\n",
        "    else:\n",
        "        count_not_present=count_not_present+1\n",
        "\n",
        "print(f\"Total Vocab in dev Eng {len(dev_set)}\")\n",
        "print(f\"Dev Vocab matching with train Eng {count_present}\")\n",
        "print(f\"Dev Vocab not matching with train Eng {count_not_present}\")\n",
        "\n",
        "#print(f\"Total vocab length in English {len(unique_tokens)}\")\n",
        "#print(f\"Total rows {row_count}\")\n",
        "\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EC1ZcANtupS",
        "outputId": "e894ae95-5340-4ec9-cd70-288ee99036d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "f2=open(\"drive/My Drive/NLP/CleanedData/cleaned_test_en\",\"rt\")\n",
        "f5=open(\"drive/My Drive/NLP/CleanedData/cleaned_test_hi\",\"rt\")\n",
        "f3=open(\"drive/My Drive/NLP/CleanedData/cleaned_test_en_withoutDups.txt\",\"w+\")\n",
        "f4=open(\"drive/My Drive/NLP/CleanedData/cleaned_test_hi_withoutDups.txt\",\"w+\")\n",
        "count_records_with_not_present_token=0\n",
        "tot_count=0\n",
        "lc = 0\n",
        "count_test = 0\n",
        "while True:\n",
        "    text=f2.readline()\n",
        "    text_hi = f5.readline()\n",
        "    if not text:\n",
        "        break\n",
        "    text=text.split()\n",
        "    text_hi = text_hi.split()\n",
        "    count_test = count_test +1\n",
        "    x= False\n",
        "    for token in text:\n",
        "        if token in not_in_train_is_in_test:\n",
        "            # print(token)\n",
        "            if x is False:\n",
        "                count_records_with_not_present_token = count_records_with_not_present_token +1\n",
        "                x = True\n",
        "            tot_count =tot_count +1\n",
        "    if x is False:\n",
        "        lc = lc + 1\n",
        "        f3.write(\" \".join(text))\n",
        "        f3.write(\"\\n\")\n",
        "        f4.write(\" \".join(text_hi))\n",
        "        f4.write(\"\\n\")          \n",
        "print(\"lc\",lc)\n",
        "\n",
        "print(f\"Test records having different tokens from train {count_records_with_not_present_token}\")\n",
        "print(f\"Total occurance of different tokens in test  {tot_count}\")\n",
        "print(f\"count test {count_test}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test records having different tokens from train 0\n",
            "Total occurance of different tokens in test  0\n",
            "count test 1694\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}